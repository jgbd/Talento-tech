{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro2cHoWc6n_E"
   },
   "source": [
    "# Simulate MLP N Layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f196124"
   },
   "source": [
    "## Define MLP structure\n",
    "\n",
    "Define the number of layers and the number of neurons in each layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756523668525,
     "user": {
      "displayName": "Juan bootcamp IA",
      "userId": "13533803708341366457"
     },
     "user_tz": 300
    },
    "id": "002adcbf"
   },
   "outputs": [],
   "source": [
    "num_layers = 3\n",
    "neurons_per_layer = [64, 128, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4b92ee55"
   },
   "source": [
    "Initialize the weight matrices and bias vectors for each layer based on the defined network structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1756523668583,
     "user": {
      "displayName": "Juan bootcamp IA",
      "userId": "13533803708341366457"
     },
     "user_tz": 300
    },
    "id": "604d41f4",
    "outputId": "8832a23c-af5c-4bff-aa4d-0aa40ce8459b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "(128, 64)\n",
      "(32, 128)\n",
      "\n",
      "Biases:\n",
      "(128, 1)\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "weights = []\n",
    "biases = []\n",
    "\n",
    "weights = [\n",
    "    np.random.randn(neurons_per_layer[i+1], neurons_per_layer[i])\n",
    "    for i in range(len(neurons_per_layer) - 1)\n",
    "]\n",
    "biases = [\n",
    "    np.random.randn(neurons_per_layer[i+1], 1)\n",
    "    for i in range(len(neurons_per_layer) - 1)\n",
    "]\n",
    "\n",
    "print(\"Weights:\")\n",
    "for w in weights:\n",
    "    print(w.shape)\n",
    "\n",
    "print(\"\\nBiases:\")\n",
    "for b in biases:\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ee4fdc08"
   },
   "source": [
    "## Defined Function for simulate MLP MultiLayer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1756523668590,
     "user": {
      "displayName": "Juan bootcamp IA",
      "userId": "13533803708341366457"
     },
     "user_tz": 300
    },
    "id": "9aff0dfe"
   },
   "outputs": [],
   "source": [
    "def forward_pass_mlp(x, weights, biases, activations):\n",
    "  \"\"\"\n",
    "  Performs the forward pass through an MLP with specified activation functions.\n",
    "\n",
    "  Args:\n",
    "    x: The input vector.\n",
    "    weights: A list of weight matrices for each layer.\n",
    "    biases: A list of bias vectors for each layer.\n",
    "    activations: A list of strings specifying the activation function for each layer.\n",
    "\n",
    "  Returns:\n",
    "    The output of the last layer.\n",
    "  \"\"\"\n",
    "  output = x\n",
    "\n",
    "  for i in range(len(weights)):\n",
    "    # Calculate the weighted sum\n",
    "    Z = np.dot(weights[i], output) + biases[i]\n",
    "\n",
    "    # Apply activation function\n",
    "    activation = activations[i]\n",
    "\n",
    "    if activation == 'linear':\n",
    "      output = Z\n",
    "    elif activation == 'relu':\n",
    "      output = np.maximum(0, Z)\n",
    "    elif activation == 'sigmoid':\n",
    "      output = 1 / (1 + np.exp(-Z))\n",
    "    elif activation == 'tanh':\n",
    "      output = np.tanh(Z)\n",
    "    else:\n",
    "      raise ValueError(f\"Unknown activation function: {activation}\")\n",
    "\n",
    "  return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8oWzMZye96zM"
   },
   "source": [
    "# Example application function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1756523668629,
     "user": {
      "displayName": "Juan bootcamp IA",
      "userId": "13533803708341366457"
     },
     "user_tz": 300
    },
    "id": "umJ5pRB8-AM7",
    "outputId": "d1307b50-000a-4210-ab66-b1ea514008d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations used: ['relu', 'relu', 'sigmoid']\n",
      "\n",
      "Final Output Shape with specified activations: (32, 1)\n",
      "\n",
      "Final Output with specified activations:\n",
      "[[9.59827767e-17]\n",
      " [9.99999999e-01]\n",
      " [1.00000000e+00]\n",
      " [1.32189484e-32]\n",
      " [1.00000000e+00]\n",
      " [1.32915638e-08]\n",
      " [7.00947276e-15]\n",
      " [1.12391899e-21]\n",
      " [1.00000000e+00]\n",
      " [4.13458259e-05]\n",
      " [9.99999999e-01]\n",
      " [2.15751430e-50]\n",
      " [2.03879018e-07]\n",
      " [1.00000000e+00]\n",
      " [9.99892802e-01]\n",
      " [5.02638603e-21]\n",
      " [1.00000000e+00]\n",
      " [1.51798396e-39]\n",
      " [3.00039303e-19]\n",
      " [8.24515686e-09]\n",
      " [1.69526104e-07]\n",
      " [5.11380611e-07]\n",
      " [1.00000000e+00]\n",
      " [9.65624603e-01]\n",
      " [9.50787785e-27]\n",
      " [5.65215678e-04]\n",
      " [1.09650252e-10]\n",
      " [1.55384149e-53]\n",
      " [9.99985953e-01]\n",
      " [1.00000000e+00]\n",
      " [6.70259969e-31]\n",
      " [1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Example usage with different activations\n",
    "activations_list = ['relu'] * (num_layers - 2) + ['sigmoid'] # Example: relu for hidden, sigmoid for output\n",
    "activations_list.insert(0, 'relu') # Assuming input layer has relu activation before the first hidden layer\n",
    "\n",
    "print(\"Activations used:\", activations_list)\n",
    "\n",
    "x = np.random.randn(neurons_per_layer[0], 1) # Example input\n",
    "final_output = forward_pass_mlp(x, weights, biases, activations_list[1:]) # Exclude input layer activation\n",
    "\n",
    "print(\"\\nFinal Output Shape with specified activations:\", final_output.shape)\n",
    "print(\"\\nFinal Output with specified activations:\")\n",
    "print(final_output)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRBmuVvxZbLPQcJN1b5NVe",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
